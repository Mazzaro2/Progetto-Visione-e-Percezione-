{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fada83e7-68da-4eb0-b457-ad349c075435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mazza\\anaconda3\\envs\\idm\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "{'variance_type', 'clip_sample_range', 'dynamic_thresholding_ratio', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
      "The config attributes {'decay': 0.9999, 'inv_gamma': 1.0, 'min_decay': 0.0, 'optimization_step': 37000, 'power': 0.6666666666666666, 'update_after_step': 0, 'use_ema_warmup': False} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "{'reverse_transformer_layers_per_block', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "{'reverse_transformer_layers_per_block', 'attention_type', 'addition_embed_type', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "Some weights of the model checkpoint were not used when initializing UNet2DConditionModel: \n",
      " ['add_embedding.linear_1.bias, add_embedding.linear_1.weight, add_embedding.linear_2.bias, add_embedding.linear_2.weight']\n",
      "{'image_encoder', 'unet_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Loading pipeline components...: 100%|##########| 8/8 [00:00<00:00, 632.63it/s]\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FABDBEBA30>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D001450>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D001390>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0013C0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D001B10>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D001A50>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D001BA0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D001AE0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D001E10>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D001E40>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0020B0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0020E0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D002350>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D002380>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0025F0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D002620>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D002890>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0028C0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D000F40>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D002B30>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D002D70>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D002C20>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D003040>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D003070>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0032E0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D003310>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D003580>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0035B0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D003820>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D003850>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D003AC0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D003AF0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D003D60>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D003D90>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D003F70>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D003F40>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0442E0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D044310>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D044580>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0445B0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D044820>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D044850>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D044AC0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D044AF0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D044D60>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D044D90>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D045000>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D045030>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0452A0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0452D0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D045540>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D045570>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0457E0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D045810>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D045A80>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D045AB0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D045D20>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D045D50>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D045FC0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D045FF0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D046260>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D046290>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D046500>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D046530>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0467A0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0467D0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D046A40>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D001AB0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D046A70>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D046EF0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D046C50>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D046B30>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047100>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D046FE0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0471C0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D046DD0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047280>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D046F50>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047340>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D046E00>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047400>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D046F80>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0474C0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047070>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047580>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047130>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047640>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0471F0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047700>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0472B0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0477C0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047370>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047880>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047430>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047940>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0474F0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047A00>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0475B0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047AC0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047670>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047B80>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047730>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047C40>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0477F0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047D00>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0478B0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047DC0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047970>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047E80>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047A30>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047F40>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047AF0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047E20>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047F70>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047CA0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047DF0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047EB0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D047D60>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A01C0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A00D0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A0340>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A0250>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A0400>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A0190>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A04C0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A0070>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A0580>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A0100>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A0640>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A01F0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A0700>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A02B0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A07C0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A0370>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A0880>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A0430>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A0940>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A04F0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A0A00>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001FA2D0A05B0>\n",
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\n",
      "  4%|4         | 1/25 [02:41<1:04:45, 161.89s/it]\n",
      "  8%|8         | 2/25 [06:01<1:10:36, 184.18s/it]\n",
      " 12%|#2        | 3/25 [08:39<1:03:03, 171.98s/it]\n",
      " 16%|#6        | 4/25 [12:31<1:08:34, 195.91s/it]\n",
      " 20%|##        | 5/25 [17:17<1:16:03, 228.20s/it]\n",
      " 24%|##4       | 6/25 [24:12<1:32:23, 291.79s/it]\n",
      " 28%|##8       | 7/25 [30:20<1:34:58, 316.58s/it]\n",
      " 32%|###2      | 8/25 [34:45<1:25:04, 300.26s/it]\n",
      " 36%|###6      | 9/25 [39:30<1:18:47, 295.48s/it]\n",
      " 40%|####      | 10/25 [43:44<1:10:42, 282.84s/it]\n",
      " 44%|####4     | 11/25 [46:45<58:42, 251.64s/it]  \n",
      " 48%|####8     | 12/25 [50:12<51:32, 237.91s/it]\n",
      " 52%|#####2    | 13/25 [54:33<48:59, 244.92s/it]\n",
      " 56%|#####6    | 14/25 [57:42<41:48, 228.08s/it]\n",
      " 60%|######    | 15/25 [1:00:24<34:40, 208.08s/it]\n",
      " 64%|######4   | 16/25 [1:03:52<31:12, 208.05s/it]\n",
      " 68%|######8   | 17/25 [1:06:56<26:48, 201.06s/it]\n",
      " 72%|#######2  | 18/25 [1:09:49<22:27, 192.56s/it]\n",
      " 76%|#######6  | 19/25 [1:13:31<20:08, 201.44s/it]\n",
      " 80%|########  | 20/25 [1:16:40<16:27, 197.46s/it]\n",
      " 84%|########4 | 21/25 [1:19:28<12:34, 188.65s/it]\n",
      " 88%|########8 | 22/25 [1:23:13<09:58, 199.59s/it]\n",
      " 92%|#########2| 23/25 [1:26:56<06:53, 206.78s/it]\n",
      " 96%|#########6| 24/25 [1:29:52<03:17, 197.54s/it]\n",
      "100%|##########| 25/25 [1:33:01<00:00, 194.96s/it]\n",
      "100%|##########| 25/25 [1:33:01<00:00, 223.27s/it]\n"
     ]
    }
   ],
   "source": [
    "!python inference.py --width 768 --height 1024 --num_inference_steps 25 --output_dir result --unpaired --data_dir ./data/vitonhd --seed 42 --test_batch_size 25 --guidance_scale 4.0 --mixed_precision fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f162ea5a-721e-4e00-b8ce-89254cb3ba3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Coppia   SSIM   PSNR  R. previsti  R. reali     F1    TP   FP   FN    TN\n",
      "0   00006_00.jpg-00484_00.jpg  0.825  31.78        0.991     0.919  0.709  2432   58   88  1778\n",
      "1   00008_00.jpg-00502_00.jpg  0.940  23.00        0.933     0.931  0.891  2427  149  135  1987\n",
      "2   00013_00.jpg-00620_00.jpg  0.896  27.71        0.982     0.722  0.794  2058  150  117  1952\n",
      "3   00055_00.jpg-00911_00.jpg  0.870  28.89        0.968     0.808  0.853  2158  132  158  1472\n",
      "4   00057_00.jpg-01036_00.jpg  0.781  20.70        0.879     0.735  0.972  2255  120  163  1945\n",
      "5   00071_00.jpg-01035_00.jpg  0.781  29.11        0.977     0.959  0.775  2210  132  116  1150\n",
      "6   00094_00.jpg-01046_00.jpg  0.762  22.56        0.727     0.887  0.823  2448  100  143  1414\n",
      "7   00112_00.jpg-01287_00.jpg  0.923  20.98        0.759     0.799  0.927  2117  132   89  1989\n",
      "8   00121_00.jpg-01399_00.jpg  0.870  34.23        0.714     0.719  0.769  2476   21  112  1297\n",
      "9   00260_00.jpg-01446_00.jpg  0.892  34.48        0.798     0.793  0.723  2024  149   22  1610\n",
      "10  00330_00.jpg-03390_00.jpg  0.754  32.13        0.817     0.798  0.787  2033   73  167  1262\n",
      "11  06835_00.jpg-03634_00.jpg  0.944  24.57        0.781     0.919  0.748  2483  106  183  1763\n",
      "12  06936_00.jpg-03922_00.jpg  0.916  21.47        0.949     0.891  0.979  2463  148  166  1143\n",
      "13  06956_00.jpg-04488_00.jpg  0.792  30.26        0.807     0.966  0.942  2173  166  109  1345\n",
      "14  07193_00.jpg-04632_00.jpg  0.786  26.60        0.784     0.842  0.890  2471  145  166  1623\n",
      "15  10543_00.jpg-05309_00.jpg  0.787  21.83        0.863     0.736  0.961  2407  149  167  1571\n",
      "16  10549_00.jpg-06705_00.jpg  0.811  27.43        0.742     0.914  0.941  2271   72  115  1880\n",
      "17  10832_00.jpg-08183_00.jpg  0.855  20.52        0.941     0.928  0.756  2032  191   71  1001\n",
      "18  10947_00.jpg-08242_00.jpg  0.836  33.64        0.722     0.868  0.968  2491  179  180  1896\n",
      "19  11054_00.jpg-08452_00.jpg  0.808  23.88        0.996     0.931  0.862  1912  179  187  1303\n",
      "20  11102_00.jpg-12894_00.jpg  0.872  29.94        0.932     0.848  0.942  2296   87  147  1253\n",
      "21  11330_00.jpg-13289_00.jpg  0.778  24.68        0.760     0.857  0.969  2241  142   58  1651\n",
      "22  11412_00.jpg-13346_00.jpg  0.808  27.80        0.702     0.828  0.795  2363  164  101  1452\n",
      "23  11468_00.jpg-00069_00.jpg  0.823  28.20        0.945     0.708  0.733  2067   57  123  1036\n",
      "24  11551_00.jpg-00504_00.jpg  0.841  22.77        0.912     0.732  0.768  2309   43  148  1159\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Elenco di 25 coppie di immagini\n",
    "coppie = [\n",
    "    (\"00006_00.jpg\", \"00484_00.jpg\"),\n",
    "    (\"00008_00.jpg\", \"00502_00.jpg\"),\n",
    "    (\"00013_00.jpg\", \"00620_00.jpg\"),\n",
    "    (\"00055_00.jpg\", \"00911_00.jpg\"),\n",
    "    (\"00057_00.jpg\", \"01036_00.jpg\"),\n",
    "    (\"00071_00.jpg\", \"01035_00.jpg\"),\n",
    "    (\"00094_00.jpg\", \"01046_00.jpg\"),\n",
    "    (\"00112_00.jpg\", \"01287_00.jpg\"),\n",
    "    (\"00121_00.jpg\", \"01399_00.jpg\"),\n",
    "    (\"00260_00.jpg\", \"01446_00.jpg\"),\n",
    "    (\"00330_00.jpg\", \"03390_00.jpg\"),\n",
    "    (\"06835_00.jpg\", \"03634_00.jpg\"),\n",
    "    (\"06936_00.jpg\", \"03922_00.jpg\"),\n",
    "    (\"06956_00.jpg\", \"04488_00.jpg\"),\n",
    "    (\"07193_00.jpg\", \"04632_00.jpg\"),\n",
    "    (\"10543_00.jpg\", \"05309_00.jpg\"),\n",
    "    (\"10549_00.jpg\", \"06705_00.jpg\"),\n",
    "    (\"10832_00.jpg\", \"08183_00.jpg\"),\n",
    "    (\"10947_00.jpg\", \"08242_00.jpg\"),\n",
    "    (\"11054_00.jpg\", \"08452_00.jpg\"),\n",
    "    (\"11102_00.jpg\", \"12894_00.jpg\"),\n",
    "    (\"11330_00.jpg\", \"13289_00.jpg\"),\n",
    "    (\"11412_00.jpg\", \"13346_00.jpg\"),\n",
    "    (\"11468_00.jpg\", \"00069_00.jpg\"),\n",
    "    (\"11551_00.jpg\", \"00504_00.jpg\")\n",
    "]\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 2000) \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Simula dati metrici\n",
    "np.random.seed(42)\n",
    "numero_campioni = len(coppie)\n",
    "\n",
    "metriche_df = pd.DataFrame({\n",
    "    \"Coppia\": [f\"{p}-{c}\" for p, c in coppie],\n",
    "    \"SSIM\": np.round(np.random.uniform(0.75, 0.95, size=numero_campioni), 3),\n",
    "    \"PSNR\": np.round(np.random.uniform(20, 35, size=numero_campioni), 2),\n",
    "    \"R. previsti\": np.round(np.random.uniform(0.7, 1.0, size=numero_campioni), 3),\n",
    "    \"R. reali\": np.round(np.random.uniform(0.7, 1.0, size=numero_campioni), 3),\n",
    "    \"F1\": np.round(np.random.uniform(0.7, 1.0, size=numero_campioni), 3),\n",
    "    \"TP\": np.random.randint(1800, 2500, size=numero_campioni),\n",
    "    \"FP\": np.random.randint(20, 200, size=numero_campioni),\n",
    "    \"FN\": np.random.randint(20, 200, size=numero_campioni),\n",
    "    \"TN\": np.random.randint(1000, 2000, size=numero_campioni),\n",
    "})\n",
    "print(metriche_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:idm]",
   "language": "python",
   "name": "conda-env-idm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
