{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e1a828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mazza\\anaconda3\\envs\\idm\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "{'variance_type', 'dynamic_thresholding_ratio', 'thresholding', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
      "The config attributes {'decay': 0.9999, 'inv_gamma': 1.0, 'min_decay': 0.0, 'optimization_step': 37000, 'power': 0.6666666666666666, 'update_after_step': 0, 'use_ema_warmup': False} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "{'dropout', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "{'attention_type', 'addition_embed_type', 'dropout', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "Some weights of the model checkpoint were not used when initializing UNet2DConditionModel: \n",
      " ['add_embedding.linear_1.bias, add_embedding.linear_1.weight, add_embedding.linear_2.bias, add_embedding.linear_2.weight']\n",
      "{'image_encoder', 'unet_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Loading pipeline components...: 100%|##########| 8/8 [00:00<00:00, 1636.64it/s]\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027B3DD7F9A0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA411420>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA411360>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA411390>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA411AE0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA411A20>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA411B70>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA411AB0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA411DE0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA411E10>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA412080>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4120B0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA412320>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA412350>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4125C0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4125F0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA412860>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA412890>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA410F10>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA412B00>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA412D40>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA412BF0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA413010>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA413040>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4132B0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4132E0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA413550>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA413580>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4137F0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA413820>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA413A90>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA413AC0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA413D30>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA413D60>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA413F40>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA413F10>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44C2B0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44C2E0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44C550>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44C580>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44C7F0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44C820>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44CA90>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44CAC0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44CD30>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44CD60>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44CFD0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44D000>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44D270>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44D2A0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44D510>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44D540>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44D7B0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44D7E0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44DA50>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44DA80>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44DCF0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44DD20>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44DF90>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44DFC0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44E230>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44E260>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44E4D0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44E500>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44E770>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44E7A0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44EA10>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA411A80>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44EA40>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44EEC0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44EC20>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44EB00>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F0D0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44EFB0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F190>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44EDA0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F250>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44EF20>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F310>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44EDD0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F3D0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44EF50>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F490>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F040>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F550>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F100>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F610>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F1C0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F6D0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F280>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F790>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F340>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F850>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F400>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F910>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F4C0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F9D0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F580>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44FA90>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F640>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44FB50>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F700>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44FC10>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F7C0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44FCD0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F880>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44FD90>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44F940>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44FE50>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44FA00>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44FF10>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44FAC0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44FB80>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44FC40>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44FFD0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44FD30>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44FE80>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44FF40>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA44FD00>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC190>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC100>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC040>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC3D0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC310>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC490>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC160>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC550>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC0D0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC610>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC1C0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC6D0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC280>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC790>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC340>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC850>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC400>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC910>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC4C0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC9D0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x0000027ABA4AC580>\n",
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\n",
      "  4%|4         | 1/25 [00:10<04:04, 10.18s/it]\n",
      "  8%|8         | 2/25 [00:18<03:30,  9.16s/it]\n",
      " 12%|#2        | 3/25 [00:27<03:14,  8.84s/it]\n",
      " 16%|#6        | 4/25 [00:35<03:00,  8.61s/it]\n",
      " 20%|##        | 5/25 [00:43<02:48,  8.44s/it]\n",
      " 24%|##4       | 6/25 [00:51<02:38,  8.33s/it]\n",
      " 28%|##8       | 7/25 [00:59<02:28,  8.25s/it]\n",
      " 32%|###2      | 8/25 [01:07<02:20,  8.26s/it]\n",
      " 36%|###6      | 9/25 [01:16<02:11,  8.24s/it]\n",
      " 40%|####      | 10/25 [01:24<02:03,  8.25s/it]\n",
      " 44%|####4     | 11/25 [01:32<01:55,  8.26s/it]\n",
      " 48%|####8     | 12/25 [01:41<01:47,  8.30s/it]\n",
      " 52%|#####2    | 13/25 [01:49<01:39,  8.31s/it]\n",
      " 56%|#####6    | 14/25 [01:57<01:31,  8.30s/it]\n",
      " 60%|######    | 15/25 [02:07<01:27,  8.71s/it]\n",
      " 64%|######4   | 16/25 [02:15<01:16,  8.53s/it]\n",
      " 68%|######8   | 17/25 [02:23<01:06,  8.36s/it]\n",
      " 72%|#######2  | 18/25 [02:31<00:58,  8.34s/it]\n",
      " 76%|#######6  | 19/25 [02:39<00:49,  8.30s/it]\n",
      " 80%|########  | 20/25 [02:48<00:41,  8.35s/it]\n",
      " 84%|########4 | 21/25 [02:56<00:33,  8.35s/it]\n",
      " 88%|########8 | 22/25 [03:05<00:25,  8.39s/it]\n",
      " 92%|#########2| 23/25 [03:13<00:16,  8.33s/it]\n",
      " 96%|#########6| 24/25 [03:22<00:08,  8.44s/it]\n",
      "100%|##########| 25/25 [03:30<00:00,  8.39s/it]\n",
      "100%|##########| 25/25 [03:30<00:00,  8.42s/it]\n",
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\n",
      "  4%|4         | 1/25 [00:07<03:03,  7.66s/it]\n",
      "  8%|8         | 2/25 [00:14<02:43,  7.12s/it]\n",
      " 12%|#2        | 3/25 [00:21<02:36,  7.10s/it]\n",
      " 16%|#6        | 4/25 [00:28<02:25,  6.94s/it]\n",
      " 20%|##        | 5/25 [00:34<02:15,  6.75s/it]\n",
      " 24%|##4       | 6/25 [00:41<02:07,  6.71s/it]\n",
      " 28%|##8       | 7/25 [00:47<02:01,  6.73s/it]\n",
      " 32%|###2      | 8/25 [00:54<01:55,  6.80s/it]\n",
      " 36%|###6      | 9/25 [01:02<01:50,  6.90s/it]\n",
      " 40%|####      | 10/25 [01:08<01:43,  6.91s/it]\n",
      " 44%|####4     | 11/25 [01:15<01:35,  6.79s/it]\n",
      " 48%|####8     | 12/25 [01:22<01:27,  6.76s/it]\n",
      " 52%|#####2    | 13/25 [01:28<01:19,  6.67s/it]\n",
      " 56%|#####6    | 14/25 [01:36<01:15,  6.89s/it]\n",
      " 60%|######    | 15/25 [01:43<01:09,  6.91s/it]\n",
      " 64%|######4   | 16/25 [01:50<01:02,  6.94s/it]\n",
      " 68%|######8   | 17/25 [01:57<00:55,  6.96s/it]\n",
      " 72%|#######2  | 18/25 [02:03<00:47,  6.83s/it]\n",
      " 76%|#######6  | 19/25 [02:10<00:41,  6.93s/it]\n",
      " 80%|########  | 20/25 [02:16<00:33,  6.65s/it]\n",
      " 84%|########4 | 21/25 [02:23<00:26,  6.64s/it]\n",
      " 88%|########8 | 22/25 [02:29<00:19,  6.54s/it]\n",
      " 92%|#########2| 23/25 [02:36<00:13,  6.64s/it]\n",
      " 96%|#########6| 24/25 [02:43<00:06,  6.69s/it]\n",
      "100%|##########| 25/25 [02:50<00:00,  6.93s/it]\n",
      "100%|##########| 25/25 [02:50<00:00,  6.83s/it]\n"
     ]
    }
   ],
   "source": [
    "!python inference.py --width 768 --height 1024 --num_inference_steps 25 --output_dir result --unpaired --data_dir ./data/vitonhd --seed 42 --test_batch_size 3 --guidance_scale 4.0 --mixed_precision fp16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "894a4fd5-510d-4849-a85a-f6f642f0880e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Coppia   SSIM   PSNR  R. previsti  R. reali     F1    TP   FP   FN    TN\n",
      "0   00006_00.jpg-00484_00.jpg  0.825  31.78        0.991     0.919  0.709  2432   58   88  1778\n",
      "1   00008_00.jpg-00502_00.jpg  0.940  23.00        0.933     0.931  0.891  2427  149  135  1987\n",
      "2   00013_00.jpg-00620_00.jpg  0.896  27.71        0.982     0.722  0.794  2058  150  117  1952\n",
      "3   00055_00.jpg-00911_00.jpg  0.870  28.89        0.968     0.808  0.853  2158  132  158  1472\n",
      "4   00057_00.jpg-01036_00.jpg  0.781  20.70        0.879     0.735  0.972  2255  120  163  1945\n",
      "5   00071_00.jpg-01035_00.jpg  0.781  29.11        0.977     0.959  0.775  2210  132  116  1150\n",
      "6   00094_00.jpg-01046_00.jpg  0.762  22.56        0.727     0.887  0.823  2448  100  143  1414\n",
      "7   00112_00.jpg-01287_00.jpg  0.923  20.98        0.759     0.799  0.927  2117  132   89  1989\n",
      "8   00121_00.jpg-01399_00.jpg  0.870  34.23        0.714     0.719  0.769  2476   21  112  1297\n",
      "9   00260_00.jpg-01446_00.jpg  0.892  34.48        0.798     0.793  0.723  2024  149   22  1610\n",
      "10  00330_00.jpg-03390_00.jpg  0.754  32.13        0.817     0.798  0.787  2033   73  167  1262\n",
      "11  06835_00.jpg-03634_00.jpg  0.944  24.57        0.781     0.919  0.748  2483  106  183  1763\n",
      "12  06936_00.jpg-03922_00.jpg  0.916  21.47        0.949     0.891  0.979  2463  148  166  1143\n",
      "13  06956_00.jpg-04488_00.jpg  0.792  30.26        0.807     0.966  0.942  2173  166  109  1345\n",
      "14  07193_00.jpg-04632_00.jpg  0.786  26.60        0.784     0.842  0.890  2471  145  166  1623\n",
      "15  10543_00.jpg-05309_00.jpg  0.787  21.83        0.863     0.736  0.961  2407  149  167  1571\n",
      "16  10549_00.jpg-06705_00.jpg  0.811  27.43        0.742     0.914  0.941  2271   72  115  1880\n",
      "17  10832_00.jpg-08183_00.jpg  0.855  20.52        0.941     0.928  0.756  2032  191   71  1001\n",
      "18  10947_00.jpg-08242_00.jpg  0.836  33.64        0.722     0.868  0.968  2491  179  180  1896\n",
      "19  11054_00.jpg-08452_00.jpg  0.808  23.88        0.996     0.931  0.862  1912  179  187  1303\n",
      "20  11102_00.jpg-12894_00.jpg  0.872  29.94        0.932     0.848  0.942  2296   87  147  1253\n",
      "21  11330_00.jpg-13289_00.jpg  0.778  24.68        0.760     0.857  0.969  2241  142   58  1651\n",
      "22  11412_00.jpg-13346_00.jpg  0.808  27.80        0.702     0.828  0.795  2363  164  101  1452\n",
      "23  11468_00.jpg-00069_00.jpg  0.823  28.20        0.945     0.708  0.733  2067   57  123  1036\n",
      "24  11551_00.jpg-00504_00.jpg  0.841  22.77        0.912     0.732  0.768  2309   43  148  1159\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "coppie = [\n",
    "    (\"00006_00.jpg\", \"00484_00.jpg\"),\n",
    "    (\"00008_00.jpg\", \"00502_00.jpg\"),\n",
    "    (\"00013_00.jpg\", \"00620_00.jpg\"),\n",
    "    (\"00055_00.jpg\", \"00911_00.jpg\"),\n",
    "    (\"00057_00.jpg\", \"01036_00.jpg\"),\n",
    "    (\"00071_00.jpg\", \"01035_00.jpg\"),\n",
    "    (\"00094_00.jpg\", \"01046_00.jpg\"),\n",
    "    (\"00112_00.jpg\", \"01287_00.jpg\"),\n",
    "    (\"00121_00.jpg\", \"01399_00.jpg\"),\n",
    "    (\"00260_00.jpg\", \"01446_00.jpg\"),\n",
    "    (\"00330_00.jpg\", \"03390_00.jpg\"),\n",
    "    (\"06835_00.jpg\", \"03634_00.jpg\"),\n",
    "    (\"06936_00.jpg\", \"03922_00.jpg\"),\n",
    "    (\"06956_00.jpg\", \"04488_00.jpg\"),\n",
    "    (\"07193_00.jpg\", \"04632_00.jpg\"),\n",
    "    (\"10543_00.jpg\", \"05309_00.jpg\"),\n",
    "    (\"10549_00.jpg\", \"06705_00.jpg\"),\n",
    "    (\"10832_00.jpg\", \"08183_00.jpg\"),\n",
    "    (\"10947_00.jpg\", \"08242_00.jpg\"),\n",
    "    (\"11054_00.jpg\", \"08452_00.jpg\"),\n",
    "    (\"11102_00.jpg\", \"12894_00.jpg\"),\n",
    "    (\"11330_00.jpg\", \"13289_00.jpg\"),\n",
    "    (\"11412_00.jpg\", \"13346_00.jpg\"),\n",
    "    (\"11468_00.jpg\", \"00069_00.jpg\"),\n",
    "    (\"11551_00.jpg\", \"00504_00.jpg\")\n",
    "]\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 2000) \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Simula dati metrici\n",
    "np.random.seed(42)\n",
    "numero_campioni = len(coppie)\n",
    "\n",
    "metriche_df = pd.DataFrame({\n",
    "    \"Coppia\": [f\"{p}-{c}\" for p, c in coppie],\n",
    "    \"SSIM\": np.round(np.random.uniform(0.75, 0.95, size=numero_campioni), 3),\n",
    "    \"PSNR\": np.round(np.random.uniform(20, 35, size=numero_campioni), 2),\n",
    "    \"R. previsti\": np.round(np.random.uniform(0.7, 1.0, size=numero_campioni), 3),\n",
    "    \"R. reali\": np.round(np.random.uniform(0.7, 1.0, size=numero_campioni), 3),\n",
    "    \"F1\": np.round(np.random.uniform(0.7, 1.0, size=numero_campioni), 3),\n",
    "    \"TP\": np.random.randint(1800, 2500, size=numero_campioni),\n",
    "    \"FP\": np.random.randint(20, 200, size=numero_campioni),\n",
    "    \"FN\": np.random.randint(20, 200, size=numero_campioni),\n",
    "    \"TN\": np.random.randint(1000, 2000, size=numero_campioni),\n",
    "})\n",
    "print(metriche_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:idm]",
   "language": "python",
   "name": "conda-env-idm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
