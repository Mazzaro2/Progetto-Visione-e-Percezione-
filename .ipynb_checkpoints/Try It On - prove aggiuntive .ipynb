{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38e1a828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mazza\\anaconda3\\envs\\idm\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "{'clip_sample_range', 'dynamic_thresholding_ratio', 'thresholding', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
      "The config attributes {'decay': 0.9999, 'inv_gamma': 1.0, 'min_decay': 0.0, 'optimization_step': 37000, 'power': 0.6666666666666666, 'update_after_step': 0, 'use_ema_warmup': False} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "{'dropout', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "{'attention_type', 'dropout', 'addition_embed_type', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "Some weights of the model checkpoint were not used when initializing UNet2DConditionModel: \n",
      " ['add_embedding.linear_1.bias, add_embedding.linear_1.weight, add_embedding.linear_2.bias, add_embedding.linear_2.weight']\n",
      "{'unet_encoder', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Loading pipeline components...: 100%|##########| 8/8 [00:00<00:00, 597.87it/s]\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001EE532CF820>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A613F0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A61330>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A61360>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A61AB0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A619F0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A61B40>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A61A80>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A61DB0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A61DE0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A62050>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A62080>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A622F0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A62320>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A62590>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A625C0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A62830>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A62860>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A60EE0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A62AD0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A62D10>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A62BC0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A62FE0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A63010>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A63280>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A632B0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A63520>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A63550>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A637C0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A637F0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A63A60>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A63A90>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A63D00>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A63D30>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A63FA0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A63FD0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA0280>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA02B0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA0520>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA0550>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA07C0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA07F0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA0A60>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA0A90>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA0D00>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA0D30>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA0FA0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA0FD0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA1240>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA1270>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA14E0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA1510>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA1780>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA17B0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA1A20>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA1A50>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA1CC0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA1CF0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA1F60>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA1F90>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA2200>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA2230>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA24A0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA24D0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA2740>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA2770>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA29E0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55A61A50>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA2A10>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA2E90>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA2BF0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA2AD0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA30A0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA2F80>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3160>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA2D70>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3220>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA2EF0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA32E0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA2DA0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA33A0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA2F20>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3460>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3010>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3520>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA30D0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA35E0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3190>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA36A0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3250>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3760>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3310>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3820>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA33D0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA38E0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3490>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA39A0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3550>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3A60>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3610>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3B20>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA36D0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3BE0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3790>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3CA0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3850>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3D60>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3910>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3E20>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA39D0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3EE0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3A90>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3B50>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3F70>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3D00>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3CD0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3FA0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3E50>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA2FE0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=640, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55AA3DC0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B000A0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B00130>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B003A0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B002E0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B00460>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B00220>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B00520>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B00160>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B005E0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B00190>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B006A0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B00250>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B00760>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B00310>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B00820>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B003D0>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B008E0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B00490>\n",
      "You are removing possibly trained weights of AttnProcessor2_0() with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B009A0>\n",
      "You are removing possibly trained weights of IPAttnProcessor2_0(\n",
      "  (to_k_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "  (to_v_ip): Linear(in_features=2048, out_features=1280, bias=False)\n",
      ") with <diffusers.models.attention_processor.SlicedAttnProcessor object at 0x000001ED55B00550>\n",
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\n",
      "  4%|4         | 1/25 [00:09<03:39,  9.16s/it]\n",
      "  8%|8         | 2/25 [00:17<03:25,  8.92s/it]\n",
      " 12%|#2        | 3/25 [00:26<03:10,  8.65s/it]\n",
      " 16%|#6        | 4/25 [00:34<03:00,  8.58s/it]\n",
      " 20%|##        | 5/25 [00:43<02:51,  8.55s/it]\n",
      " 24%|##4       | 6/25 [00:51<02:40,  8.47s/it]\n",
      " 28%|##8       | 7/25 [00:59<02:32,  8.45s/it]\n",
      " 32%|###2      | 8/25 [01:08<02:22,  8.37s/it]\n",
      " 36%|###6      | 9/25 [01:16<02:13,  8.36s/it]\n",
      " 40%|####      | 10/25 [01:24<02:04,  8.31s/it]\n",
      " 44%|####4     | 11/25 [01:32<01:55,  8.28s/it]\n",
      " 48%|####8     | 12/25 [01:41<01:48,  8.31s/it]\n",
      " 52%|#####2    | 13/25 [01:49<01:40,  8.37s/it]\n",
      " 56%|#####6    | 14/25 [01:58<01:32,  8.43s/it]\n",
      " 60%|######    | 15/25 [02:07<01:25,  8.53s/it]\n",
      " 64%|######4   | 16/25 [02:15<01:16,  8.52s/it]\n",
      " 68%|######8   | 17/25 [02:24<01:08,  8.50s/it]\n",
      " 72%|#######2  | 18/25 [02:32<00:59,  8.54s/it]\n",
      " 76%|#######6  | 19/25 [02:41<00:51,  8.59s/it]\n",
      " 80%|########  | 20/25 [02:50<00:43,  8.63s/it]\n",
      " 84%|########4 | 21/25 [02:59<00:34,  8.73s/it]\n",
      " 88%|########8 | 22/25 [03:07<00:26,  8.70s/it]\n",
      " 92%|#########2| 23/25 [03:16<00:17,  8.70s/it]\n",
      " 96%|#########6| 24/25 [03:25<00:08,  8.68s/it]\n",
      "100%|##########| 25/25 [03:33<00:00,  8.66s/it]\n",
      "100%|##########| 25/25 [03:33<00:00,  8.55s/it]\n"
     ]
    }
   ],
   "source": [
    "!python inference.py --width 768 --height 1024 --num_inference_steps 25 --output_dir result --unpaired --data_dir ./data/vitonhd --seed 42 --test_batch_size 3 --guidance_scale 4.0 --mixed_precision fp16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bed5767-f9bd-4f85-8e34-6d66092b4410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Coppia   SSIM   PSNR  R. previsti  R. reali     F1    TP   FP   FN    TN\n",
      "0   00006_00.jpg-00484_00.jpg  0.825  31.78        0.991     0.919  0.709  2432   58   88  1778\n",
      "1   00008_00.jpg-00502_00.jpg  0.940  23.00        0.933     0.931  0.891  2427  149  135  1987\n",
      "2   00013_00.jpg-00620_00.jpg  0.896  27.71        0.982     0.722  0.794  2058  150  117  1952\n",
      "3   00055_00.jpg-00911_00.jpg  0.870  28.89        0.968     0.808  0.853  2158  132  158  1472\n",
      "4   00057_00.jpg-01036_00.jpg  0.781  20.70        0.879     0.735  0.972  2255  120  163  1945\n",
      "5   00071_00.jpg-01035_00.jpg  0.781  29.11        0.977     0.959  0.775  2210  132  116  1150\n",
      "6   00094_00.jpg-01046_00.jpg  0.762  22.56        0.727     0.887  0.823  2448  100  143  1414\n",
      "7   00112_00.jpg-01287_00.jpg  0.923  20.98        0.759     0.799  0.927  2117  132   89  1989\n",
      "8   00121_00.jpg-01399_00.jpg  0.870  34.23        0.714     0.719  0.769  2476   21  112  1297\n",
      "9   00260_00.jpg-01446_00.jpg  0.892  34.48        0.798     0.793  0.723  2024  149   22  1610\n",
      "10  00330_00.jpg-03390_00.jpg  0.754  32.13        0.817     0.798  0.787  2033   73  167  1262\n",
      "11  06835_00.jpg-03634_00.jpg  0.944  24.57        0.781     0.919  0.748  2483  106  183  1763\n",
      "12  06936_00.jpg-03922_00.jpg  0.916  21.47        0.949     0.891  0.979  2463  148  166  1143\n",
      "13  06956_00.jpg-04488_00.jpg  0.792  30.26        0.807     0.966  0.942  2173  166  109  1345\n",
      "14  07193_00.jpg-04632_00.jpg  0.786  26.60        0.784     0.842  0.890  2471  145  166  1623\n",
      "15  10543_00.jpg-05309_00.jpg  0.787  21.83        0.863     0.736  0.961  2407  149  167  1571\n",
      "16  10549_00.jpg-06705_00.jpg  0.811  27.43        0.742     0.914  0.941  2271   72  115  1880\n",
      "17  10832_00.jpg-08183_00.jpg  0.855  20.52        0.941     0.928  0.756  2032  191   71  1001\n",
      "18  10947_00.jpg-08242_00.jpg  0.836  33.64        0.722     0.868  0.968  2491  179  180  1896\n",
      "19  11054_00.jpg-08452_00.jpg  0.808  23.88        0.996     0.931  0.862  1912  179  187  1303\n",
      "20  11102_00.jpg-12894_00.jpg  0.872  29.94        0.932     0.848  0.942  2296   87  147  1253\n",
      "21  11330_00.jpg-13289_00.jpg  0.778  24.68        0.760     0.857  0.969  2241  142   58  1651\n",
      "22  11412_00.jpg-13346_00.jpg  0.808  27.80        0.702     0.828  0.795  2363  164  101  1452\n",
      "23  11468_00.jpg-00069_00.jpg  0.823  28.20        0.945     0.708  0.733  2067   57  123  1036\n",
      "24  11551_00.jpg-00504_00.jpg  0.841  22.77        0.912     0.732  0.768  2309   43  148  1159\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "coppie = [\n",
    "    (\"00006_00.jpg\", \"00484_00.jpg\"),\n",
    "    (\"00008_00.jpg\", \"00502_00.jpg\"),\n",
    "    (\"00013_00.jpg\", \"00620_00.jpg\"),\n",
    "    (\"00055_00.jpg\", \"00911_00.jpg\"),\n",
    "    (\"00057_00.jpg\", \"01036_00.jpg\"),\n",
    "    (\"00071_00.jpg\", \"01035_00.jpg\"),\n",
    "    (\"00094_00.jpg\", \"01046_00.jpg\"),\n",
    "    (\"00112_00.jpg\", \"01287_00.jpg\"),\n",
    "    (\"00121_00.jpg\", \"01399_00.jpg\"),\n",
    "    (\"00260_00.jpg\", \"01446_00.jpg\"),\n",
    "    (\"00330_00.jpg\", \"03390_00.jpg\"),\n",
    "    (\"06835_00.jpg\", \"03634_00.jpg\"),\n",
    "    (\"06936_00.jpg\", \"03922_00.jpg\"),\n",
    "    (\"06956_00.jpg\", \"04488_00.jpg\"),\n",
    "    (\"07193_00.jpg\", \"04632_00.jpg\"),\n",
    "    (\"10543_00.jpg\", \"05309_00.jpg\"),\n",
    "    (\"10549_00.jpg\", \"06705_00.jpg\"),\n",
    "    (\"10832_00.jpg\", \"08183_00.jpg\"),\n",
    "    (\"10947_00.jpg\", \"08242_00.jpg\"),\n",
    "    (\"11054_00.jpg\", \"08452_00.jpg\"),\n",
    "    (\"11102_00.jpg\", \"12894_00.jpg\"),\n",
    "    (\"11330_00.jpg\", \"13289_00.jpg\"),\n",
    "    (\"11412_00.jpg\", \"13346_00.jpg\"),\n",
    "    (\"11468_00.jpg\", \"00069_00.jpg\"),\n",
    "    (\"11551_00.jpg\", \"00504_00.jpg\")\n",
    "]\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 2000) \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Simula dati metrici\n",
    "np.random.seed(42)\n",
    "numero_campioni = len(coppie)\n",
    "\n",
    "metriche_df = pd.DataFrame({\n",
    "    \"Coppia\": [f\"{p}-{c}\" for p, c in coppie],\n",
    "    \"SSIM\": np.round(np.random.uniform(0.75, 0.95, size=numero_campioni), 3),\n",
    "    \"PSNR\": np.round(np.random.uniform(20, 35, size=numero_campioni), 2),\n",
    "    \"R. previsti\": np.round(np.random.uniform(0.7, 1.0, size=numero_campioni), 3),\n",
    "    \"R. reali\": np.round(np.random.uniform(0.7, 1.0, size=numero_campioni), 3),\n",
    "    \"F1\": np.round(np.random.uniform(0.7, 1.0, size=numero_campioni), 3),\n",
    "    \"TP\": np.random.randint(1800, 2500, size=numero_campioni),\n",
    "    \"FP\": np.random.randint(20, 200, size=numero_campioni),\n",
    "    \"FN\": np.random.randint(20, 200, size=numero_campioni),\n",
    "    \"TN\": np.random.randint(1000, 2000, size=numero_campioni),\n",
    "})\n",
    "print(metriche_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:idm]",
   "language": "python",
   "name": "conda-env-idm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
